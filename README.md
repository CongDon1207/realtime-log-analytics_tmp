# realtime-log-analytics

Giáº£i phÃ¡p **giÃ¡m sÃ¡t & phÃ¢n tÃ­ch log real-time** cho web servers:

* Thu tháº­p log tá»« nhiá»u mÃ¡y chá»§ báº±ng **Apache Flume**
* Váº­n chuyá»ƒn qua **Apache Kafka**
* PhÃ¢n tÃ­ch real-time báº±ng **Apache Spark (Structured Streaming)**
* LÆ°u **time-series** vÃ o **InfluxDB**
* Trá»±c quan hÃ³a & cáº£nh bÃ¡o báº±ng **Grafana**

## âœ¨ TÃ­nh nÄƒng chÃ­nh

* Äáº¿m **HTTP status** (2xx/3xx/4xx/5xx), **RPS**, **latency**, **byte**
* PhÃ¡t hiá»‡n **báº¥t thÆ°á»ng**: *IP spike*, *error surge*, *scan nhiá»u URL*
* **Dashboard** real-time vÃ  **Alerting** theo ngÆ°á»¡ng
* Thiáº¿t káº¿ **má»Ÿ rá»™ng ngang**, **Ä‘á»™ trá»… tháº¥p**, **chá»‹u lá»—i**

---
## ğŸ—ï¸ Kiáº¿n trÃºc

<div align="center">
  <img src="docs/architecture.jpeg" alt="System Architecture" width="820"><br/>
  <em>HÃ¬nh 1. Kiáº¿n trÃºc tá»•ng thá»ƒ</em>
</div>

---
## ğŸ“Š Luá»“ng dá»¯ liá»‡u log

<div align="center">
  <img src="docs/log-pipeline.jpg" alt="Log Data Pipeline" width="1000"><br/>
  <em>HÃ¬nh 2. Luá»“ng dá»¯ liá»‡u tá»« Nginx log Ä‘áº¿n Grafana</em>
</div>

---

## ğŸ§° CÃ´ng nghá»‡ sá»­ dá»¥ng

* **Apache Flume** â€“ thu tháº­p log (TAILDIR, Avro/Kafka sink)
* **Apache Kafka** â€“ message broker, buffer, decoupling producer/consumer
* **Apache Spark (Structured Streaming)** â€“ xá»­ lÃ½ luá»“ng, cá»­a sá»• thá»i gian
* **InfluxDB** â€“ cÆ¡ sá»Ÿ dá»¯ liá»‡u time-series
* **Grafana** â€“ dashboard & alerting
* **Docker / Docker Compose** â€“ mÃ´i trÆ°á»ng triá»ƒn khai

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```text
realtime-log-analytics/
â”œâ”€ docker-compose.yml
â”œâ”€ deploy.sh
â”œâ”€ .env
â”œâ”€ README.md
â”‚
â”œâ”€ flume/
â”‚  â”œâ”€ agents/
â”‚  â”‚  â”œâ”€ web1/flume.conf
â”‚  â”‚  â”œâ”€ web2/flume.conf
â”‚  â”‚  â””â”€ web3/flume.conf
â”‚  â””â”€ collector/flume.conf
â”‚
â”œâ”€ kafka/
â”‚  â”œâ”€ config/server.properties    # (optional, override)
â”‚  â””â”€ make_topics.sh              # táº¡o topic web-logs, partitions, retention
â”‚
â”œâ”€ spark/
â”‚  â”œâ”€ src/main/scala/com/yourproject/LogAnalyzer.scala  # (hoáº·c python/)
â”‚  â”œâ”€ conf/app.yaml              # window, watermark, thresholds
â”‚  â”œâ”€ tests/test_parsing.scala   # (hoáº·c pytest náº¿u dÃ¹ng PySpark)
â”‚  â”œâ”€ build.sbt                  # hoáº·c pom.xml / requirements.txt
â”‚  â””â”€ Dockerfile
â”‚
â”œâ”€ influxdb/
â”‚  â”œâ”€ config/influxdb.conf       # (optional)
â”‚  â””â”€ init/                      # script táº¡o org/bucket/token, retention
â”‚
â”œâ”€ grafana/
â”‚  â”œâ”€ provisioning/
â”‚  â”‚  â”œâ”€ datasources/datasource.yml
â”‚  â”‚  â””â”€ dashboards/dashboards.yml
â”‚  â””â”€ dashboards/
â”‚     â”œâ”€ log-analysis-dashboard.json
â”‚     â””â”€ alerts.json             # (optional) export alert rules
â”‚
â”œâ”€ scripts/
â”‚  â”œâ”€ log-generator/log_generator.py
â”‚  â”œâ”€ gen_traffic.sh             # ab/hey/wrk
â”‚  â””â”€ smoke.sh                   # kiá»ƒm tra nhanh Kafka/Influx/Grafana
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ logs/web1/access.log
â”‚  â”œâ”€ logs/web2/access.log
â”‚  â”œâ”€ logs/web3/access.log
â”‚  â””â”€ state/spark/checkpoints/
â”‚
â””â”€ docs/
   â”œâ”€ architecture.md
   â””â”€ runbook.md

```


---

## ğŸ‘¥ Chia nhiá»‡m vá»¥ (5 ngÆ°á»i)

* **HÃ¹ng (Flume & Deploy): ğŸš€**
  Phá»¥ trÃ¡ch Ä‘áº§u vÃ o (gom log) vÃ  triá»ƒn khai dá»± Ã¡n lÃªn server.

  * Viáº¿t & cáº¥u hÃ¬nh Flume Agents (3 server) + Aggregator/Collector (náº¿u dÃ¹ng 2 táº§ng)
  * Thiáº¿t láº­p `TAILDIR`, `file channel`, Avro/Kafka sink, xá»­ lÃ½ log rotate
  * Soáº¡n `deploy.sh`, chuáº©n bá»‹ `.env`, Ä‘áº£m báº£o stack cháº¡y á»•n Ä‘á»‹nh
  * **Deliverables:** `flume.conf` (agents/aggregator), `deploy.sh`, hÆ°á»›ng dáº«n triá»ƒn khai

* **Háº£o (Kafka): ğŸ”—**
  Äáº£m báº£o â€œÄ‘Æ°á»ng á»‘ngâ€ váº­n chuyá»ƒn log hoáº¡t Ä‘á»™ng tá»‘t.

  * Cáº¥u hÃ¬nh Kafka (topic `web-logs`, partitions, retention, acks)
  * Theo dÃµi consumer lag, runbook start/stop, script táº¡o topic
  * (TÃ¹y chá»n) KRaft (khÃ´ng ZK) cho Compose demo
  * **Deliverables:** `kafka/config/server.properties`, `make_topics.sh`, runbook Kafka

* **Háº£i (Spark): ğŸ§ **
  XÃ¢y dá»±ng â€œbá»™ nÃ£oâ€ phÃ¢n tÃ­ch dá»¯ liá»‡u.

  * Structured Streaming Ä‘á»c Kafka â†’ parse log, tÃ­nh RPS & Ä‘áº¿m status theo **window 10s**, **watermark 2m**
  * Kiá»ƒm soÃ¡t throughput/backpressure, checkpointing
  * **Deliverables:** mÃ£ nguá»“n `spark/src/...`, build script, test parsing

* **ÄÃ´n (InfluxDB): ğŸ’¾**
  Phá»¥ trÃ¡ch â€œnhÃ  khoâ€ lÆ°u trá»¯ káº¿t quáº£.

  * Thiáº¿t káº¿ bucket/retention, measurement (`http_stats`, `anomaly`)
  * Giao tiáº¿p tá»« Spark (`foreachBatch`/HTTP client), tá»‘i Æ°u batch write
  * **Deliverables:** `influxdb/config/influxdb.conf` (náº¿u cáº§n), schema & hÆ°á»›ng dáº«n token/ORG/BUCKET

* **Nháº­t (Grafana): âœ…**
  Phá»¥ trÃ¡ch â€œmáº·t tiá»nâ€ (dashboard) vÃ  quáº£n lÃ½ chung.

  * Provision datasource & dashboards, thiáº¿t káº¿ panel (RPS, 4xx/5xx%, Top IP/URL, báº£ng anomaly)
  * Thiáº¿t láº­p alert rules (ip\_spike, error\_surge)
  * **Deliverables:** `grafana/provisioning/*`, `dashboards/*.json`, README.md

---

### Quy Æ°á»›c NhÃ¡nh (Branching Convention)

* **NhÃ¡nh chÃ­nh:**
    * `main`: Chá»©a phiÃªn báº£n á»•n Ä‘á»‹nh, sáºµn sÃ ng Ä‘á»ƒ deploy.
    * `develop`: NhÃ¡nh tÃ­ch há»£p chÃ­nh. Táº¥t cáº£ cÃ¡c nhÃ¡nh `feature` sáº½ Ä‘Æ°á»£c merge vÃ o Ä‘Ã¢y sau khi hoÃ n thÃ nh.

* **Quy Æ°á»›c Ä‘áº·t tÃªn nhÃ¡nh `feature`:**
    * `feat/flume-ingestion`: **HÃ¹ng** (Flume)
    * `feat/kafka-pipeline`: **Háº£o** (Kafka)
    * `feat/spark-analytics`: **Háº£i** (Spark)
    * `feat/influxdb-storage`: **ÄÃ´n** (InfluxDB)
    * `feat/grafana-dashboard`: **Nháº­t** (Grafana)
## ğŸ“š TÃ i liá»‡u chi tiáº¿t

- ğŸ“„ **Project Doc (Google Docs)**: [TÃ i liá»‡u Kiáº¿n trÃºc â€“ Há»‡ thá»‘ng GiÃ¡m sÃ¡t & PhÃ¢n tÃ­ch Log Server Táº­p trung](https://docs.google.com/document/d/1PiGJ2ZUnI4yse3WgkTP1DghxmsFws_z2dIaZyuNYdq8/edit?hl=vi&tab=t.0#heading=h.exjyajopfano)


# H∆∞·ªõng d·∫´n Pipeline: Nginx ‚Üí Flume ‚Üí Kafka

## T·ªïng quan
Pipeline n√†y thu th·∫≠p logs t·ª´ c√°c web servers Nginx, s·ª≠ d·ª•ng Apache Flume ƒë·ªÉ stream d·ªØ li·ªáu, v√† ƒë∆∞a v√†o Apache Kafka ƒë·ªÉ l∆∞u tr·ªØ v√† x·ª≠ l√Ω.

## Lu·ªìng d·ªØ li·ªáu

```
Nginx Servers (web1, web2, web3) 
   ‚Üì (Log files: access.json.log, error.log)
Flume Agents (taildir source) 
   ‚Üì (Avro sink ‚Üí port 41414)
Flume Collector (multiplexing selector)
   ‚Üì (Kafka producer)
Kafka Topics (web-logs, web-errors)
   ‚Üì (Consumed by)
Spark Structured Streaming (access ‚Üí metrics / anomaly)
   ‚Üì (Writes metrics)
InfluxDB (bucket: logs ‚Äî measurements: http_stats, top_urls, anomaly)
```

## C√°c b∆∞·ªõc kh·ªüi ch·∫°y logs ƒë∆∞a v√†o kafka

### 1. Kh·ªüi t·∫°o to√†n b·ªô container
```bash
# Kh·ªüi ƒë·ªông to√†n b·ªô container
docker compose up -d 

# Ki·ªÉm tra Kafka ƒë√£ running
docker compose ps
```

### 2. T·∫°o Topics Kafka
```bash
# T·∫°o (ho·∫∑c recreate) c√°c topic b·∫±ng script helper c√≥ s·∫µn trong container
# Script s·∫Ω x√≥a (n·∫øu c√≥) r·ªìi t·∫°o l·∫°i `web-logs` v√† `web-errors` m·ªôt c√°ch an to√†n.
bash kafka/create-topic2.sh

# Xem danh s√°ch topics
docker exec -it kafka bash -c "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list"

# Xem th√¥ng tin chi ti·∫øt topic
docker exec -it kafka bash -c "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --describe --topic web-logs"
```

### L∆∞u √Ω: ch·ªó n√†y kh√¥ng c·∫ßn l√†m c≈©ng ƒë∆∞·ª£c, do error log ƒë√£ t·ª± t·∫°o r·ªìi
### 3. Generate Access Logs (Th·ªß c√¥ng) 
```bash
# T·∫°o 10 requests loi ƒë·∫øn /api endpoint  
for i in {1..10}; do curl -s -o /dev/null -w "%{http_code}\n" http://localhost:8081/api; done
```

### 4. Generate Error Logs (Th·ªß c√¥ng)
```bash
# T·∫°o 5 requests l·ªói ƒë·∫øn /oops endpoint
for i in {1..5}; do curl -s -o /dev/null -w "%{http_code}\n" http://localhost:8081/oops; done
```

### 5. Ki·ªÉm tra Consumer

#### Consumer cho Access Logs (web-logs topic):
```bash
docker exec -it kafka bash -c "/opt/bitnami/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic web-logs --from-beginning"
```

#### Consumer cho Error Logs (web-errors topic):
```bash
docker exec -it kafka bash -c "/opt/bitnami/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic web-errors --from-beginning"
```

## C·∫•u tr√∫c d·ªØ li·ªáu

### Access Logs (JSON format):
```json
{
  "time": "2025-09-09T02:43:12+00:00",
  "ts": 1757385792.958,
  "remote": "172.22.0.1", 
  "hostname": "web1",
  "method": "GET",
  "path": "/api",
  "status": 502,
  "bytes": 157,
  "rt": 0.001
}
```

### Error Logs (Plain text format):
```
2025/09/09 02:43:12 [error] 33#33: *276 connect() failed (111: Connection refused) while connecting to upstream, client: 172.22.0.1, server: , request: "GET /api HTTP/1.1", upstream: "http://127.0.0.1:65535/api", host: "localhost:8081"
```



## Spark Structured Streaming (access ‚Üí metrics/anomaly ‚Üí InfluxDB)

### B∆∞·ªõc 1: Kh·ªüi ƒë·ªông InfluxDB
```bash
# Kh·ªüi t·∫°o org/bucket (ch·ªâ ch·∫°y 1 l·∫ßn) - script s·∫Ω ƒë·ªçc c·∫•u h√¨nh t·ª´ .env
bash influxdb/init/onboarding.sh
```

#### ‚ö†Ô∏è L∆∞u √Ω quan tr·ªçng cho l·∫ßn ƒë·∫ßu setup InfluxDB:

> **Ch·ªâ th·ª±c hi·ªán 1 l·∫ßn duy nh·∫•t** khi kh·ªüi t·∫°o InfluxDB l·∫ßn ƒë·∫ßu ti√™n.

**C√°c b∆∞·ªõc th·ª±c hi·ªán:**

1. **Truy c·∫≠p InfluxDB UI**: M·ªü tr√¨nh duy·ªát v√† v√†o `http://localhost:8086`

2. **ƒêƒÉng nh·∫≠p h·ªá th·ªëng**:
   - Username: `admin`  
   - Password: `admin12345`

![image alt](https://github.com/hungfnguyen/realtime-log-analytics/blob/feat/influxdb-storage/docs/img/influxdb_login.jpg?raw=true)
3. **T·∫°o API Token m·ªõi**:
   - Sau khi ƒëƒÉng nh·∫≠p, click v√†o **bi·ªÉu t∆∞·ª£ng m≈©i t√™n ‚Üó** (Load Data) ·ªü sidebar tr√°i
   - Ch·ªçn **API Tokens** t·ª´ menu
    ![image alt](https://github.com/hungfnguyen/realtime-log-analytics/blob/feat/influxdb-storage/docs/img/guide_influxdb.png?raw=true)

   - **X√≥a token c≈©** (n·∫øu c√≥) b·∫±ng c√°ch click v√†o token v√† ch·ªçn Delete
    ![image alt](https://github.com/hungfnguyen/realtime-log-analytics/blob/feat/influxdb-storage/docs/img/delete_api_token.jpg?raw=true)
    
   - Click **Generate API Token** ‚Üí **All Access API Token**
   - ƒê·∫∑t t√™n cho token (v√≠ d·ª•: `spark-streaming-token`)
   - **Copy token** v·ª´a ƒë∆∞·ª£c t·∫°o

4. **C·∫≠p nh·∫≠t file c·∫•u h√¨nh**:
   - M·ªü file `.env` ·ªü th∆∞ m·ª•c root c·ªßa project
   - Thay th·∫ø gi√° tr·ªã `INFLUX_TOKEN` b·∫±ng token v·ª´a copy

   ```bash
   # V√≠ d·ª• format trong file .env:
   INFLUX_TOKEN=DHxxYj3F83RYX4vZwj7Ftebb1jpKJnR0ylu96ZGH9BvvQT3hkmPs9V73r6c3uOKpS2fulZ76DlYnmFlL9rFLqQ==
   ```
   ![image alt](https://github.com/hungfnguyen/realtime-log-analytics/blob/feat/influxdb-storage/docs/img/config_env.jpg?raw=true)
    
5. **Ki·ªÉm tra k·∫øt n·ªëi**: 
   ```bash
   source .env && curl -I -H "Authorization: Token $INFLUX_TOKEN" http://localhost:8086/ping
   ```

> üí° **Ghi ch√∫**: Token n√†y s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi Spark streaming job ƒë·ªÉ ghi d·ªØ li·ªáu v√†o InfluxDB. Kh√¥ng chia s·∫ª token n√†y v·ªõi ng∆∞·ªùi kh√°c.
### B∆∞·ªõc 2: Ki·ªÉm tra c·∫•u h√¨nh InfluxDB
```bash
# Ki·ªÉm tra InfluxDB API c√≥ s·∫µn s√†ng
curl -I http://localhost:8086/ping

# Ki·ªÉm tra c·∫•u h√¨nh t·ª´ file .env
source .env && echo "ORG: $INFLUX_ORG, BUCKET: $INFLUX_BUCKET"

# Ki·ªÉm tra c·∫•u h√¨nh InfluxDB t·ª´ .env
source .env && echo "INFLUX_TOKEN: ${INFLUX_TOKEN:0:20}..."
```

### B∆∞·ªõc 3: Ch·∫°y Spark Streaming job (access log)
```bash
# L·ªánh ng·∫Øn g·ªçn (t·ª± n·∫°p .env, ch·∫°y 70‚Äì75s r·ªìi d·ª´ng)
bash scripts/run_access_stream.sh

#Ho·∫∑c √©p timeout:
set -a; . .env; set +a; docker exec -e INFLUX_URL -e INFLUX_TOKEN -e INFLUX_ORG -e INFLUX_BUCKET -e ENV_TAG -e WINDOW_DURATION -e WATERMARK -e CHECKPOINT_DIR spark-master bash -lc 'timeout 75s /opt/bitnami/spark/bin/spark-submit --master spark://spark-master-influx:7077 /opt/spark/app/src/python/stream_access.py'
```

### B∆∞·ªõc 4: Ch·∫°y Spark Streaming job (error log)
```bash
# L·ªánh ng·∫Øn g·ªçn (t·ª± n·∫°p .env, ch·∫°y 70‚Äì75s r·ªìi d·ª´ng)
bash scripts/run_error_stream.sh

#Ho·∫∑c √©p timeout:
set -a; . .env; set +a; docker exec -e INFLUX_URL -e INFLUX_TOKEN -e INFLUX_ORG -e INFLUX_BUCKET -e ENV_TAG -e WINDOW_DURATION -e WATERMARK -e CHECKPOINT_DIR_ERROR spark-master bash -lc 'timeout 75s /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 /opt/spark/app/src/python/stream_error.py'
```


### Output v√† Measurements InfluxDB

Pipeline s·∫Ω t·∫°o ra 4 measurements ch√≠nh trong InfluxDB ƒë·ªÉ ph√¢n t√≠ch logs v√† monitoring:

#### 1. `http_stats` - Metrics hi·ªáu nƒÉng HTTP theo c·ª≠a s·ªï th·ªùi gian
- **M·ª•c ƒë√≠ch**: Theo d√µi hi·ªáu nƒÉng v√† t√¨nh tr·∫°ng c·ªßa web servers
- **Tags**: `env` (m√¥i tr∆∞·ªùng), `hostname` (t√™n server), `method` (HTTP method)
- **Fields**: 
  - `count`: T·ªïng s·ªë requests trong window
  - `rps`: Requests per second
  - `avg_rt`: Response time trung b√¨nh (ms)
  - `max_rt`: Response time t·ªëi ƒëa (ms)  
  - `err_rate`: T·ª∑ l·ªá l·ªói (%)
- **Time**: `window_end` (k·∫øt th√∫c c·ª≠a s·ªï 10 gi√¢y)

#### 2. `top_urls` - Top URLs ƒë∆∞·ª£c truy c·∫≠p nhi·ªÅu nh·∫•t
- **M·ª•c ƒë√≠ch**: Ph√¢n t√≠ch traffic patterns v√† endpoints ph·ªï bi·∫øn
- **Tags**: `env`, `hostname`, `status` (HTTP status code), `path` (URL path)
- **Fields**: `count` (s·ªë l·∫ßn truy c·∫≠p)
- **Time**: `window_end`

#### 3. `anomaly` - Ph√°t hi·ªán b·∫•t th∆∞·ªùng trong traffic
- **M·ª•c ƒë√≠ch**: C·∫£nh b√°o c√°c h√†nh vi b·∫•t th∆∞·ªùng (DDoS, bot attacks, etc.)
- **Tags**: `env`, `hostname`, `kind` (lo·∫°i b·∫•t th∆∞·ªùng: ip_spike, rate_limit, etc.)
- **Fields**: 
  - `ip`: IP address c√≥ h√†nh vi b·∫•t th∆∞·ªùng
  - `count`: S·ªë requests t·ª´ IP ƒë√≥
  - `score`: ƒêi·ªÉm b·∫•t th∆∞·ªùng (0-1, c√†ng cao c√†ng nghi ng·ªù)
- **Time**: `window_end`

#### 4. `error_events` - Th·ªëng k√™ l·ªói h·ªá th·ªëng theo c·ª≠a s·ªï th·ªùi gian  
- **M·ª•c ƒë√≠ch**: Theo d√µi v√† ph√¢n lo·∫°i c√°c l·ªói h·ªá th·ªëng
- **Tags**: `env`, `hostname`, `level` (ERROR/WARN/CRIT), `message_class` (db/api/network/etc.)
- **Fields**: `count` (s·ªë l∆∞·ª£ng l·ªói trong window)
- **Ghi ch√∫**: `hostname` lu√¥n l√† `web1`/`web2`/`web3`; n·∫øu log g·ªëc thi·∫øu gi√° tr·ªã s·∫Ω ƒë∆∞·ª£c thay b·∫±ng `unknown_host`
- **Time**: `window_end`

### Ki·ªÉm tra d·ªØ li·ªáu trong InfluxDB

#### Truy c·∫≠p InfluxDB UI:
```bash
echo "InfluxDB UI: http://localhost:8086 (Org: primary, Bucket: logs)"
```

#### Query t·ªïng quan t·∫•t c·∫£ measurements:
```bash
# Xem to√†n b·ªô d·ªØ li·ªáu (gi·ªõi h·∫°n 10 records)
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> limit(n: 10)' --org $INFLUX_ORG --token $INFLUX_TOKEN

# ƒê·∫øm t·ªïng s·ªë records trong t·∫•t c·∫£ measurements
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> count()' --org $INFLUX_ORG --token $INFLUX_TOKEN
```

#### Query HTTP Performance Metrics (`http_stats`):
```bash
# Xem metrics hi·ªáu nƒÉng HTTP v·ªõi pivot ƒë·ªÉ d·ªÖ ƒë·ªçc
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "http_stats") |> pivot(rowKey:["_time","hostname","method"], columnKey: ["_field"], valueColumn: "_value")' --org $INFLUX_ORG --token $INFLUX_TOKEN

# Ch·ªâ xem response time metrics
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "http_stats" and (r._field == "avg_rt" or r._field == "max_rt")) |> pivot(rowKey:["_time","hostname","method"], columnKey: ["_field"], valueColumn: "_value")' --org $INFLUX_ORG --token $INFLUX_TOKEN
```

#### Check ph√¢n ph·ªëi latency (rt) ‚Äì ch·∫©n ƒëo√°n nhanh
```bash
source .env && docker exec influxdb influx query '
  a = from(bucket: "logs")
    |> range(start: -30m)
    |> filter(fn: (r) => r._measurement == "http_stats" and r._field == "avg_rt")
    |> quantile(q: 0.95, method: "exact_selector")

  b = from(bucket: "logs")
    |> range(start: -30m)
    |> filter(fn: (r) => r._measurement == "http_stats" and r._field == "max_rt")
    |> quantile(q: 0.95, method: "exact_selector")

  union(tables: {avg_rt_p95: a, max_rt_p95: b})
' --org $INFLUX_ORG --token $INFLUX_TOKEN
```

#### Query Top URLs (`top_urls`):
```bash
# Top URLs ƒë∆∞·ª£c truy c·∫≠p nhi·ªÅu nh·∫•t
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "top_urls" and r._field == "count") |> group(columns: ["hostname", "path", "status"]) |> sum(column: "_value") |> sort(columns: ["_value"], desc: true) |> limit(n:10)' --org $INFLUX_ORG --token $INFLUX_TOKEN

# URLs c√≥ status code l·ªói (4xx, 5xx)
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "top_urls" and r._field == "count" and (r.status =~ /^[45]/)) |> group(columns: ["hostname", "path", "status"]) |> sum(column: "_value") |> sort(columns: ["_value"], desc: true) |> limit(n:10)' --org $INFLUX_ORG --token $INFLUX_TOKEN
```

#### Query Anomaly Detection (`anomaly`):
```bash
# T·∫•t c·∫£ b·∫•t th∆∞·ªùng ƒë∆∞·ª£c ph√°t hi·ªán
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "anomaly") |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")' --org $INFLUX_ORG --token $INFLUX_TOKEN

# Ch·ªâ c√°c b·∫•t th∆∞·ªùng c√≥ score cao (> 0.7)
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "anomaly" and r._field == "score" and r._value > 0.7)' --org $INFLUX_ORG --token $INFLUX_TOKEN
```

#### Query Error Events (`error_events`):
```bash
# T·∫•t c·∫£ l·ªói h·ªá th·ªëng v·ªõi pivot
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "error_events") |> pivot(rowKey:["_time","hostname","level"], columnKey: ["_field"], valueColumn: "_value")' --org $INFLUX_ORG --token $INFLUX_TOKEN

# Ch·ªâ l·ªói CRITICAL v√† ERROR level  
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "error_events" and r.level == "error") |> sort(columns: ["_time"], desc: true)' --org $INFLUX_ORG --token $INFLUX_TOKEN

# Th·ªëng k√™ l·ªói theo hostname v√† level
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "error_events") |> group(columns: ["hostname", "level"]) |> sum()' --org $INFLUX_ORG --token $INFLUX_TOKEN
```

#### X√≥a d·ªØ li·ªáu logs sau khi s·ª≠ d·ª•ng xong
```bash
   # X√≥a log files tr√™n disk
   find data/logs -type f -not -name ".gitkeep" -delete
   # ho·∫∑c s·∫°ch lu√¥n (c·∫©n th·∫≠n):
   rm -rf data/logs/web{1,2,3}/*

   # X√≥a d·ªØ li·ªáu trong Kafka topics (Git Bash tr√™n Windows)
   # C√°ch 1 (khuy√™n d√πng): d√πng helper script trong repo ‚Äî script s·∫Ω ch·ªù broker r·ªìi recreate topics s·∫°ch
   bash kafka/create-topic.sh

   # C√°ch 2: X√≥a t·ª´ng topic r·ªìi t·∫°o l·∫°i (non-interactive, ph√π h·ª£p v·ªõi Git Bash)
   docker compose exec -T kafka bash -lc "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --delete --topic web-logs"
   docker compose exec -T kafka bash -lc "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --delete --topic web-errors"
   docker compose exec -T kafka bash -lc "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --replication-factor 1 --partitions 1 --topic web-logs"
   docker compose exec -T kafka bash -lc "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --replication-factor 1 --partitions 1 --topic web-errors"
```

### Troubleshooting

#### L·ªói k·∫øt n·ªëi Spark Worker:
```bash
# Restart l·∫°i Spark cluster
docker-compose -f docker-compose.spark.yml down
docker-compose -f docker-compose.spark.yml up -d

# Ki·ªÉm tra network v√† services
docker network ls | grep appnet
docker-compose -f docker-compose.spark.yml ps
```

#### L·ªói InfluxDB connection:
```bash
# Ki·ªÉm tra InfluxDB service
docker-compose -f docker-compose.don.yml ps influxdb

# Ki·ªÉm tra logs InfluxDB
docker logs influxdb | tail -20

# Test k·∫øt n·ªëi th·ªß c√¥ng
curl -I http://localhost:8086/ping

# Ki·ªÉm tra token v√† org
source .env && echo "Token length: ${#INFLUX_TOKEN}, Org: $INFLUX_ORG"
```

#### Kh√¥ng c√≥ d·ªØ li·ªáu trong InfluxDB:
```bash
# Ki·ªÉm tra Kafka c√≥ messages
docker exec kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic web-logs --max-messages 5 --timeout-ms 5000

# Ki·ªÉm tra Spark job c√≥ ch·∫°y v√† x·ª≠ l√Ω d·ªØ li·ªáu
docker logs spark-master 2>/dev/null | grep -E "(SUCCESS.*Wrote|stream_access.py|MicroBatchExecution)"

# Ki·ªÉm tra environment variables ƒë∆∞·ª£c truy·ªÅn ƒë√∫ng
docker exec spark-master env | grep -E "(INFLUX|WINDOW|CHECKPOINT)"

# Test manual write v√†o InfluxDB
source .env && echo "test_measurement,tag1=value1 field1=123i $(date +%s)000000000" | docker exec -i influxdb influx write --bucket $INFLUX_BUCKET --org $INFLUX_ORG --token $INFLUX_TOKEN
```



### Example output khi pipeline ho·∫°t ƒë·ªông
```
# T·ª´ Spark logs
SUCCESS: Wrote 6 lines to InfluxDB bucket 'logs'
SUCCESS: Wrote 5 lines to InfluxDB bucket 'logs' 
SUCCESS: Wrote 3 lines to InfluxDB bucket 'logs'

# T·ª´ InfluxDB query
Table: keys: [_start, _stop, _field, _measurement, env, hostname, method]
http_stats | env=it-check | hostname=web1 | method=GET | avg_rt=0.133 | count=3 | rps=0.3

Table: keys: [_start, _stop, _field, _measurement, env, hostname, kind]  
anomaly | env=it-check | hostname=web1 | kind=ip_spike | ip=2.2.2.2 | count=3 | score=1.0
```


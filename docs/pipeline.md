# H∆∞·ªõng d·∫´n Pipeline: Nginx ‚Üí Flume ‚Üí Kafka

## T·ªïng quan
Pipeline n√†y thu th·∫≠p logs t·ª´ c√°c web servers Nginx, s·ª≠ d·ª•ng Apache Flume ƒë·ªÉ stream d·ªØ li·ªáu, v√† ƒë∆∞a v√†o Apache Kafka ƒë·ªÉ l∆∞u tr·ªØ v√† x·ª≠ l√Ω.

## Lu·ªìng d·ªØ li·ªáu
```
Nginx Servers (web1, web2, web3) 
    ‚Üì (Log files: access.json.log, error.log)
Flume Agents (taildir source) 
    ‚Üì (Avro sink ‚Üí port 41414)
Flume Collector (multiplexing selector)
    ‚Üì (Kafka producer)
Kafka Topics (web-logs, web-errors)
```

```
Nginx Servers (web1, web2, web3) 
   ‚Üì (Log files: access.json.log, error.log)
Flume Agents (taildir source) 
   ‚Üì (Avro sink ‚Üí port 41414)
Flume Collector (multiplexing selector)
   ‚Üì (Kafka producer)
Kafka Topics (web-logs, web-errors)
   ‚Üì (Consumed by)
Spark Structured Streaming (access ‚Üí metrics / anomaly)
   ‚Üì (Writes metrics)
InfluxDB (bucket: logs ‚Äî measurements: http_stats, top_urls, anomaly)
```

## C√°c b∆∞·ªõc kh·ªüi ch·∫°y Pipeline
### 0. T·∫°o network tr∆∞·ªõc
```bash
docker network create appnet
```

### 1. Kh·ªüi ƒë·ªông Kafka
```bash
# Kh·ªüi ƒë·ªông Kafka service
docker-compose -f docker-compose.hao.yml up -d

# Ki·ªÉm tra Kafka ƒë√£ running
docker-compose -f docker-compose.hao.yml ps
```

### 2. T·∫°o Topics Kafka
```bash
# T·∫°o (ho·∫∑c recreate) c√°c topic b·∫±ng script helper c√≥ s·∫µn trong container
# Script s·∫Ω x√≥a (n·∫øu c√≥) r·ªìi t·∫°o l·∫°i `web-logs` v√† `web-errors` m·ªôt c√°ch an to√†n.
bash kafka/create-topic2.sh

# Xem danh s√°ch topics
docker exec -it kafka bash -c "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list"
```

### 3. Kh·ªüi ƒë·ªông Nginx Servers
```bash
# Kh·ªüi ƒë·ªông 3 web servers (web1, web2, web3)
docker-compose -f docker-compose.nginx.yml up -d

# Ki·ªÉm tra services
docker-compose -f docker-compose.nginx.yml ps
```

### 4. Kh·ªüi ƒë·ªông Flume Services
```bash
# Kh·ªüi ƒë·ªông Flume collector v√† agents
docker-compose -f docker-compose.flume.yml up -d

# Ki·ªÉm tra Flume services
docker-compose -f docker-compose.flume.yml ps
```

## Test Pipeline

### 1. Generate Access Logs
```bash
# T·∫°o 10 requests loi ƒë·∫øn /api endpoint
for i in {1..10}; do curl -s -o /dev/null -w "%{http_code}\n" http://localhost:8081/api; done
```

### 2. Generate Error Logs  
```bash
# T·∫°o 5 requests l·ªói ƒë·∫øn /oops endpoint
for i in {1..5}; do curl -s -o /dev/null -w "%{http_code}\n" http://localhost:8081/oops; done
```

### 3. Ki·ªÉm tra Consumer

#### Consumer cho Access Logs (web-logs topic):
```bash
docker exec -it kafka bash -c "/opt/bitnami/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic web-logs --from-beginning"
```

#### Consumer cho Error Logs (web-errors topic):
```bash
docker exec -it kafka bash -c "/opt/bitnami/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic web-errors --from-beginning"
```

## C·∫•u tr√∫c d·ªØ li·ªáu

### Access Logs (JSON format):
```json
{
  "time": "2025-09-09T02:43:12+00:00",
  "ts": 1757385792.958,
  "remote": "172.22.0.1", 
  "hostname": "5062a08432a6",
  "method": "GET",
  "path": "/api",
  "status": 502,
  "bytes": 157,
  "rt": 0.001
}
```

### Error Logs (Plain text format):
```
2025/09/09 02:43:12 [error] 33#33: *276 connect() failed (111: Connection refused) while connecting to upstream, client: 172.22.0.1, server: , request: "GET /api HTTP/1.1", upstream: "http://127.0.0.1:65535/api", host: "localhost:8081"
```

## Ki·ªÉm tra tr·∫°ng th√°i



### Ki·ªÉm tra c√°c Nginx container c√≥ th·ªÉ t·∫°o logs:
```bash
docker exec web1 sh -c "tail -5 /var/log/nginx/access.json.log"
docker exec web2 sh -c "tail -5 /var/log/nginx/access.json.log"  
docker exec web3 sh -c "tail -5 /var/log/nginx/access.json.log"

# Ki·ªÉm tra error logs
docker exec web1 sh -c "tail -5 /var/log/nginx/error.log"
docker exec web2 sh -c "tail -5 /var/log/nginx/error.log"
docker exec web3 sh -c "tail -5 /var/log/nginx/error.log"
```



### Ki·ªÉm tra Kafka topics:
```bash
# Li·ªát k√™ topics
docker exec -it kafka bash -c "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list"

# Xem th√¥ng tin chi ti·∫øt topic
docker exec -it kafka bash -c "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --describe --topic web-logs"
```

## Troubleshooting

### L·ªói th∆∞·ªùng g·∫∑p:

1. **Network kh√¥ng t·ªìn t·∫°i**: 
   ```bash
   docker network create appnet
   ```

2. **Topic ƒë√£ t·ªìn t·∫°i**:
   - L·ªói n√†y kh√¥ng ·∫£nh h∆∞·ªüng, topic v·∫´n s·ª≠ d·ª•ng ƒë∆∞·ª£c b√¨nh th∆∞·ªùng

3. **Consumer kh√¥ng nh·∫≠n ƒë∆∞·ª£c data**:
   - Ki·ªÉm tra Flume services ƒëang ch·∫°y: `docker-compose -f docker-compose.flume.yml ps`
   - Ki·ªÉm tra Kafka service: `docker-compose -f docker-compose.hao.yml ps`  
   - Generate th√™m test data b·∫±ng curl

4. **502 Bad Gateway khi test**:
   - ƒê√¢y l√† l·ªói mong mu·ªën ƒë·ªÉ test error logs
   - Nginx proxy ƒë·∫øn backend kh√¥ng t·ªìn t·∫°i (port 65535)

## K·∫øt lu·∫≠n
Pipeline ho·∫°t ƒë·ªông th√†nh c√¥ng v·ªõi lu·ªìng:
- ‚úì Nginx t·∫°o access/error logs
- ‚úì Flume agents ƒë·ªçc logs t·ª´ files
- ‚úì Flume collector nh·∫≠n data t·ª´ agents  
- ‚úì Data ƒë∆∞·ª£c g·ª≠i v√†o Kafka topics
- ‚úì Consumer c√≥ th·ªÉ ƒë·ªçc real-time data t·ª´ Kafka

---

## Spark Structured Streaming (access ‚Üí metrics/anomaly ‚Üí InfluxDB)

### B∆∞·ªõc 1: Kh·ªüi ƒë·ªông InfluxDB
```bash
# Kh·ªüi ƒë·ªông InfluxDB service
docker-compose -f docker-compose.don.yml up -d

# Ki·ªÉm tra InfluxDB ƒë√£ ch·∫°y
docker-compose -f docker-compose.don.yml ps influxdb

# Kh·ªüi t·∫°o org/bucket (ch·ªâ ch·∫°y 1 l·∫ßn) - script s·∫Ω ƒë·ªçc c·∫•u h√¨nh t·ª´ .env
bash influxdb/init/onboarding.sh
```

#### ‚ö†Ô∏è L∆∞u √Ω quan tr·ªçng cho l·∫ßn ƒë·∫ßu setup InfluxDB:

> **Ch·ªâ th·ª±c hi·ªán 1 l·∫ßn duy nh·∫•t** khi kh·ªüi t·∫°o InfluxDB l·∫ßn ƒë·∫ßu ti√™n.

**C√°c b∆∞·ªõc th·ª±c hi·ªán:**

1. **Truy c·∫≠p InfluxDB UI**: M·ªü tr√¨nh duy·ªát v√† v√†o `http://localhost:8086`

2. **ƒêƒÉng nh·∫≠p h·ªá th·ªëng**:
   - Username: `admin`  
   - Password: `admin12345`

![image alt](https://github.com/hungfnguyen/realtime-log-analytics/blob/feat/influxdb-storage/docs/img/influxdb_login.jpg?raw=true)
3. **T·∫°o API Token m·ªõi**:
   - Sau khi ƒëƒÉng nh·∫≠p, click v√†o **bi·ªÉu t∆∞·ª£ng m≈©i t√™n ‚Üó** (Load Data) ·ªü sidebar tr√°i
   - Ch·ªçn **API Tokens** t·ª´ menu
    ![image alt](https://github.com/hungfnguyen/realtime-log-analytics/blob/feat/influxdb-storage/docs/img/guide_influxdb.png?raw=true)

   - **X√≥a token c≈©** (n·∫øu c√≥) b·∫±ng c√°ch click v√†o token v√† ch·ªçn Delete
    ![image alt](https://github.com/hungfnguyen/realtime-log-analytics/blob/feat/influxdb-storage/docs/img/delete_api_token.jpg?raw=true)
    
   - Click **Generate API Token** ‚Üí **All Access API Token**
   - ƒê·∫∑t t√™n cho token (v√≠ d·ª•: `spark-streaming-token`)
   - **Copy token** v·ª´a ƒë∆∞·ª£c t·∫°o

4. **C·∫≠p nh·∫≠t file c·∫•u h√¨nh**:
   - M·ªü file `.env` ·ªü th∆∞ m·ª•c root c·ªßa project
   - Thay th·∫ø gi√° tr·ªã `INFLUX_TOKEN` b·∫±ng token v·ª´a copy

   ```bash
   # V√≠ d·ª• format trong file .env:
   INFLUX_TOKEN=DHxxYj3F83RYX4vZwj7Ftebb1jpKJnR0ylu96ZGH9BvvQT3hkmPs9V73r6c3uOKpS2fulZ76DlYnmFlL9rFLqQ==
   ```
   ![image alt](https://github.com/hungfnguyen/realtime-log-analytics/blob/feat/influxdb-storage/docs/img/config_env.jpg?raw=true)
    
5. **Ki·ªÉm tra k·∫øt n·ªëi**: 
   ```bash
   source .env && curl -I -H "Authorization: Token $INFLUX_TOKEN" http://localhost:8086/ping
   ```

> üí° **Ghi ch√∫**: Token n√†y s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi Spark streaming job ƒë·ªÉ ghi d·ªØ li·ªáu v√†o InfluxDB. Kh√¥ng chia s·∫ª token n√†y v·ªõi ng∆∞·ªùi kh√°c.
### B∆∞·ªõc 2: Ki·ªÉm tra c·∫•u h√¨nh InfluxDB
```bash
# Ki·ªÉm tra InfluxDB API c√≥ s·∫µn s√†ng
curl -I http://localhost:8086/ping

# Ki·ªÉm tra c·∫•u h√¨nh t·ª´ file .env
source .env && echo "ORG: $INFLUX_ORG, BUCKET: $INFLUX_BUCKET"

# Ki·ªÉm tra c·∫•u h√¨nh InfluxDB t·ª´ .env
source .env && echo "INFLUX_TOKEN: ${INFLUX_TOKEN:0:20}..."
```

### B∆∞·ªõc 3: Kh·ªüi ch·∫°y Spark cluster
```bash
# Kh·ªüi ƒë·ªông Spark cluster (master + worker)
docker-compose -f docker-compose.spark.yml up -d

# Ki·ªÉm tra Spark cluster
docker-compose -f docker-compose.spark.yml ps
docker logs spark-master | tail -10
```

### B∆∞·ªõc 4: Ch·∫°y Spark Streaming job
```bash
# L·ªánh ng·∫Øn g·ªçn (t·ª± n·∫°p .env, ch·∫°y 70‚Äì75s r·ªìi d·ª´ng)
bash scripts/run_access_stream.sh

#Ho·∫∑c √©p timeout:
set -a; . .env; set +a; docker exec -e INFLUX_URL -e INFLUX_TOKEN -e INFLUX_ORG -e INFLUX_BUCKET -e ENV_TAG -e WINDOW_DURATION -e WATERMARK -e CHECKPOINT_DIR spark-master bash -lc 'timeout 75s /opt/bitnami/spark/bin/spark-submit --master spark://spark-master-influx:7077 /opt/spark/app/src/python/stream_access.py'
```



### Output v√† measurements InfluxDB

Pipeline s·∫Ω t·∫°o ra 3 measurements trong InfluxDB:

#### 1. `http_stats` - Th·ªëng k√™ HTTP theo c·ª≠a s·ªï th·ªùi gian
- **Tags**: `env`, `hostname`, `method` 
- **Fields**: `count` (t·ªïng requests), `rps` (requests/second), `avg_rt` (response time trung b√¨nh), `max_rt` (response time t·ªëi ƒëa), `err_rate` (t·ª∑ l·ªá l·ªói %)
- **Time**: `window_end` (k·∫øt th√∫c c·ª≠a s·ªï 10 gi√¢y)

#### 2. `top_urls` - Top URLs ƒë∆∞·ª£c truy c·∫≠p nhi·ªÅu nh·∫•t
- **Tags**: `env`, `hostname`, `status`, `path`
- **Fields**: `count` (s·ªë l·∫ßn truy c·∫≠p)
- **Time**: `window_end`

#### 3. `anomaly` - Ph√°t hi·ªán b·∫•t th∆∞·ªùng
- **Tags**: `env`, `hostname`, `kind` (lo·∫°i b·∫•t th∆∞·ªùng)
- **Fields**: `ip` (IP address), `count` (s·ªë requests), `score` (ƒëi·ªÉm b·∫•t th∆∞·ªùng)
- **Time**: `window_end`




#### Ki·ªÉm tra d·ªØ li·ªáu trong InfluxDB (sang terminal kh√°c):
```bash
# Truy c·∫≠p InfluxDB UI
echo "InfluxDB UI: http://localhost:8086 (Org: primary, Bucket: logs)"

# Query d·ªØ li·ªáu t·ª´ command line
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> limit(n: 10)' --org $INFLUX_ORG --token $INFLUX_TOKEN

# Query http_stats v·ªõi pivot ƒë·ªÉ xem metrics
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "http_stats") |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")' --org $INFLUX_ORG --token $INFLUX_TOKEN

# Count t·ªïng s·ªë records
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> count()' --org $INFLUX_ORG --token $INFLUX_TOKEN
```

#### Query measurements c·ª• th·ªÉ:
```bash
# Query anomaly detection results
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "anomaly") |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")' --org $INFLUX_ORG --token $INFLUX_TOKEN

# Query top URLs
source .env && docker exec influxdb influx query 'from(bucket: "logs") |> range(start: -1h) |> filter(fn: (r) => r._measurement == "top_urls") |> sort(columns: ["count"], desc: true) |> limit(n: 10)' --org $INFLUX_ORG --token $INFLUX_TOKEN
```

### Troubleshooting

#### L·ªói k·∫øt n·ªëi Spark Worker:
```bash
# Restart l·∫°i Spark cluster
docker-compose -f docker-compose.spark.yml down
docker-compose -f docker-compose.spark.yml up -d

# Ki·ªÉm tra network v√† services
docker network ls | grep appnet
docker-compose -f docker-compose.spark.yml ps
```

#### L·ªói InfluxDB connection:
```bash
# Ki·ªÉm tra InfluxDB service
docker-compose -f docker-compose.don.yml ps influxdb

# Ki·ªÉm tra logs InfluxDB
docker logs influxdb | tail -20

# Test k·∫øt n·ªëi th·ªß c√¥ng
curl -I http://localhost:8086/ping

# Ki·ªÉm tra token v√† org
source .env && echo "Token length: ${#INFLUX_TOKEN}, Org: $INFLUX_ORG"
```

#### Kh√¥ng c√≥ d·ªØ li·ªáu trong InfluxDB:
```bash
# Ki·ªÉm tra Kafka c√≥ messages
docker exec kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic web-logs --max-messages 5 --timeout-ms 5000

# Ki·ªÉm tra Spark job c√≥ ch·∫°y v√† x·ª≠ l√Ω d·ªØ li·ªáu
docker logs spark-master 2>/dev/null | grep -E "(SUCCESS.*Wrote|stream_access.py|MicroBatchExecution)"

# Ki·ªÉm tra environment variables ƒë∆∞·ª£c truy·ªÅn ƒë√∫ng
docker exec spark-master env | grep -E "(INFLUX|WINDOW|CHECKPOINT)"

# Test manual write v√†o InfluxDB
source .env && echo "test_measurement,tag1=value1 field1=123i $(date +%s)000000000" | docker exec -i influxdb influx write --bucket $INFLUX_BUCKET --org $INFLUX_ORG --token $INFLUX_TOKEN
```

### Ghi ch√∫ performance
- **Package pre-configured**: `spark.jars.packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0` trong `spark/conf/spark-defaults.conf` gi√∫p kh√¥ng c·∫ßn `--packages` flag
- **Native libraries**: ƒê√£ c√†i `libsnappy-dev` trong Spark Docker image ƒë·ªÉ tr√°nh l·ªói compression
- **InfluxDB client**: `influxdb-client==1.35.0` ƒë∆∞·ª£c install s·∫µn trong runtime environment
- Checkpoint ƒë∆∞·ª£c l∆∞u t·∫°i `/tmp/checkpoints/{stats,top_urls,anomaly}` ƒë·ªÉ ƒë·∫£m b·∫£o fault tolerance

### Ghi ch√∫ thi·∫øt k·∫ø
- **Window & Watermark**: 10 gi√¢y v·ªõi watermark 2 ph√∫t ƒë·ªÉ x·ª≠ l√Ω late events
- **Metrics calculation**: 
  - `err_rate` t√≠nh tr√™n t·ªïng requests theo (hostname, method) m·ªói window
  - `rps` = count / window_duration_seconds
  - `anomaly` detection v·ªõi thresholds: IP spike ‚â•50, scan ‚â•20 paths, error rate ‚â•10%
- **Data format**: InfluxDB line protocol v·ªõi proper tag/field separation v√† nanosecond timestamps
- **Scalability**: Top URLs t√≠nh s·∫µn theo (hostname, status, path), filtering Top-N ·ªü visualization layer

### Performance metrics t·ª´ test
- **Processing latency**: ~2-3 gi√¢y cho m·ªói micro-batch 10s window
- **Throughput**: X·ª≠ l√Ω ƒë∆∞·ª£c ~5 events/second v·ªõi 3 parallel streaming queries
- **Memory usage**: Spark driver ~434MB, worker t√πy thu·ªôc workload
- **InfluxDB write**: Batch size 500 records, flush interval 1s

### Example output khi pipeline ho·∫°t ƒë·ªông
```
# T·ª´ Spark logs
SUCCESS: Wrote 6 lines to InfluxDB bucket 'logs'
SUCCESS: Wrote 5 lines to InfluxDB bucket 'logs' 
SUCCESS: Wrote 3 lines to InfluxDB bucket 'logs'

# T·ª´ InfluxDB query
Table: keys: [_start, _stop, _field, _measurement, env, hostname, method]
http_stats | env=it-check | hostname=web1 | method=GET | avg_rt=0.133 | count=3 | rps=0.3

Table: keys: [_start, _stop, _field, _measurement, env, hostname, kind]  
anomaly | env=it-check | hostname=web1 | kind=ip_spike | ip=2.2.2.2 | count=3 | score=1.0
```
